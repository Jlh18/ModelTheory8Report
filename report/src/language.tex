\link{model_theory_background}
This section will introduce the main tools of model theory.
We will introduce \code{lean} terminology and syntax in tandem.

\subsection{Languages}
\begin{dfn}[Language]
  A language (also known as a \textit{signature}) \texttt{ L = (functions, relations) } consists of

  \begin{itemize}
    \item A sort symbol $A$, which we will have in the background for intuition.
    \item For each natural number $n$ we have \texttt{functions n} -
          the collection of \textit{function symbols} of \textit{arity} $n$ for the language.
          For some function symbol $f$ of arity $n$ we might write
          $f : A^{n} \to A$ to denote $f$ with its arity.
    \item For each natural number $n$ we have \texttt{relations n} -
          the collection of \textit{relation symbols} of \textit{arity} $n$ for the language.
          For some relation symbol $r$ of arity $n$ we might write
          $r \hookr A^{n}$ to denote $r$ with its arity.
  \end{itemize}

  The \texttt{flypitch} project implements the above definition as

  \begin{lstlisting}
  structure Language : Type (u+1) :=
    (functions : ℕ → Type u)
    (relations : ℕ → Type u)\end{lstlisting}

  This says that \texttt{Language} is a mathematical structure
  (like a group structure, or ring structure)
  that consists of two pieces of data,
  a map called \texttt{functions} and another called \texttt{relations}.
  Both take a natural number and spit out a \textit{type}
  (in set theory a \textit{set})
  that consists respectively of all the function symbols and relation symbols
  of arity $n$.

  In more detail: in type theory when we write \texttt{a:A} we mean \texttt{a} is something
  of \textit{type} \texttt{A}.
  With caution, we can draw an analogy with the set theoretic notion $a \in A$.
  Note that in the above definitions, the two types
  \texttt{functions n} and \texttt{relations n} are things of type \texttt{Type u},
  where \texttt{Type u} is a collection of all types at level \texttt{u}.
  ``Types are of type \texttt{Type u}.''

  For convenience we single out $0$-ary (arity $0$) functions and
  call them \textit{constant} symbols, usually denoting them by $c : A$.
  We think of these as `elements' of the sort $A$ and write $c : A$.
  This is defined in \texttt{lean} by

  \begin{lstlisting}
  def constants (L : Language) : Type u := functions 0\end{lstlisting}

  This says that \texttt{constants} takes in a language \texttt{L} and returns a type.
  Following the \texttt{:=} we have the definition of \texttt{constants L},
  which is the type \texttt{functions 0}.
\end{dfn}

\begin{eg}
  The \linkto{dfn_rings}{language of rings}
  will be used to define the \linkto{ring_theory}{theory of rings},
  the theory of integral domains, the theory of fields, and so on.
  In more generality we can make the language of groups and
  theory of groups, Abelian groups, cyclic groups and so on.

  In the appendix we provide the example of the
  \linkto{dfn_bin_rel}{language with just a single binary relation}.
  This can be used to define the theory of graphs by taking edges as the binary relation,
  to define the theory of partial orders with $<$ as the binary relation,
  to define the theory of equivalence relations with $\sim$ as the binary relation,
  and to define the theory $\ZFC$ with $\in$ as the binary relation.

  Another interesting example is fixing a ring $A$ and making
  the \linkto{dfn_module}{language of modules over $A$},
  with which we can define the theory of modules.
  In the same vein, we can consider the language of actions from
  a particular monoid $M$ or a group $G$.
\end{eg}

We will only be concerned with the language of rings and will
focus our examples around this.

\begin{dfn}[Language of rings]
    \link{dfn_rings}
    Let the following be the language of rings:
    \begin{itemize}
        \item The function symbols are the constant symbols $0, 1 : A$,
        $+ , \times : A^2 \to A$ for addition and multiplication
        and $- : A \to A$ for taking additive inverse.
        \item There are no relation symbols.
    \end{itemize}

    We can break this definition up into steps in \texttt{lean}.
    We first collect the constant, unary and binary symbols:

    \begin{lstlisting}
      /-- The constant symbols in RingLanguage -/
      inductive ring_consts : Type u
      | zero : ring_consts
      | one : ring_consts

      /-- The unary function symbols in RingLanguage-/
      inductive ring_unaries : Type u
      | neg : ring_unaries

      /-- The binary function symbols in RingLanguage-/
      inductive ring_binaries : Type u
      | add : ring_binaries
      | mul : ring_binaries\end{lstlisting}

    These are \textit{inductive types}\footnote{
    In the appendix we give more examples of inductive types
    \begin{itemize}
      \item The \linkto{nat_dfn}{natural numbers} are defined as inductive types
      \item \linkto{list_dfn}{Lists} are defined as inductive types
    \end{itemize}
    We soon introduce further inductive types such as
    \linkto{terms_dfn}{terms in a language} and \linkto{formulas_dfn}{formulas in a language}. } -
    types that are `freely' generated by their constructors,
    which are listed after each bar `\texttt{|}'.
    For example, the definition of \texttt{ring\_consts} reads
    ``the only things of type \texttt{ring\_consts} are
    \texttt{ring\_consts.zero} and \texttt{ring\_consts.one}''.

    We now collect all the above into a single definition \code{ring\_funcs}
    that takes each natural \code{n} to the type of \code{n}-ary
    function symbols in the language of rings.
    The syntax below is casing (induction) on the naturals,

    \begin{lstlisting}
      /-- All function symbols in RingLanguage-/
      def ring_funcs : ℕ → Type u
      | 0 := ring_consts
      | 1 := ring_unaries
      | 2 := ring_binaries
      | (n + 3) := pempty\end{lstlisting}

    The type \code{pempty} is the empty type and is meant to have no terms in it,
    since we wish to have no function symbols beyond arity $2$.
    Finally we make the language of rings

    \begin{lstlisting}
      /-- The language of rings -/
      def ring_language : Language :=
      (Language.mk) (ring_funcs) (λ n, pempty)\end{lstlisting}
\end{dfn}

We use languages to express logical assertions, such as
``any (at most) degree two polynomial over my ring has a root''
(in preparation for expressing algebraic closure).
In order to do so we must introduce terms (polynomials in our case),
formulas (assertions about rings), structures (``prerings'') and models (ring),
and the relation between structures and formulas
(whether the ring satisfies this assertion).

We want to write down all sensible combinations of symbols (and variables)
we can make in a language.
We can think of multi-variable polynomials over the integers as such:
the only sensible combinations of symbols $0,1,-,+,*$ and variables
are elements of $\Z{[x_{k}]}_{k \in \N}$,
where $x_{k}$ are the variables. We formalize this generally as terms.

\subsection{Terms and formulas}

\begin{dfn}[Terms]
  \link{terms_dfn}
  Let \code{ L = (functions, relations) } be a language.
  To make a (bounded) \textit{preterm} in \code{L} with up to $n$ variables
  we can do one of three things:
  \begin{itemize}
    \item[$\vert$] For each natural number $k < n$ we create a symbol
          $x_{k}$, which we call a \textit{variable} in $A$.
          Any $x_{k}$ is a preterm (that is missing nothing).
    \item[$\vert$] If $f : A^{l} \to A$ is a function symbol then
          $f$ is a preterm that is missing $l$ inputs.
          \[ f( ? , \cdots , ? )\]
    \item[$\vert$] If $t$ is a preterm that is missing
          $l + 1$ inputs and $s$ is a preterm that is missing
          no inputs then we can \textit{apply} $t$ to $s$, obtaining
          a preterm that is missing $l$ inputs.
          \[ t(s , ? , \cdots, ? )\]
  \end{itemize}

  We only really want \textit{terms} with up to $n$ variables,
  which are defined as preterms that are missing nothing.

  \begin{lstlisting}
    inductive bounded_preterm (n : ℕ) : ℕ → Type u
    | x_ : ∀ (k : fin n), bounded_preterm 0
    | bd_func : ∀ {l : ℕ} (f : L.functions l), bounded_preterm l
    | bd_app : ∀ {l : ℕ} (t : bounded_preterm (l + 1)) (s : bounded_preterm 0), bounded_preterm l

    def bounded_term (n : ℕ) := bounded_preterm L n 0\end{lstlisting}

  To explain notation
  \begin{itemize}
    \item The second constructor says ``for all naturals $l$ and function symbols $f$,
          \code{bd\_func f} is something of type \code{bounded\_preterm l}''.
          This makes sense since \code{bounded\_preterm l} is a type by the first line of code.
    \item The curly brackets just say
          ``you can leave out this input and \code{lean} will know what it is''.
  \end{itemize}

  To give an example of this in action we can write $x_{1} * 0$.
  We first write the individual parts, which are
  \code{x\_ 1}, \code{bd\_func mul} and
  \code{bd\_func zero}.
  \code{bd\_func mul} is a preterm missing $2$ inputs;
  applying \code{mul} to \code{x\_ 1} gives us a preterm \code{bd\_app (bd\_func mul) (x\_ 1)}
  missing $1$ input.
  Then applying this to \code{bd\_func zero} gives a term
  \begin{lstlisting}
    bd_app (bd_app (bd_func mul) (x_ 1)) (bd_func zero) \end{lstlisting}
  We will introduce nice notation in \code{lean} to make this more legible.
\end{dfn}

\begin{rmk}
  There is an unfortunate terminology clash between model theory and type theory,
  since they are closely related.
  The word ``term'' in type theory refers to anything on the left of a \code{:} sign;
  ``a term of type \code{A}''.
  In this document we say ``something of type \code{A}'' to avoid confusion.

  Terms in inductively defined types are (as mentioned before)
  freely generated symbols using the constructors.
  Similarly, terms in a language are freely generated symbols using
  the symbols from the language.
\end{rmk}

One can imagine writing down any degree two polynomial over the integers
as a term in the language of rings.
In fact, we could even make degree two polynomials over any ring (if we had one):
\[ x_{0} x_{3}^{2} + x_{1} x_{3} + x_{2} \]
Here our variable is $x_{3}$, and we imagine that the other variables represent
elements of our ring.
To express ``any degree (up to) two polynomial over our ring has a root'',
we are only missing logical connectives:
\[ \forall x_{2} \, x_{1} \, x_{0} : A, \exists x_{3} : A, x_{0} x_{3}^{2} + x_{1} x_{3} + x_{2} = 0 \]
This will be expressed as a formula.

\begin{dfn}[Formulas]
  \link{formulas_dfn}
  Let \code{L} be a language.
  A (classical first order bounded) \code{L}-\textit{preformula} in \code{L}
  with (up to) $n$ \textit{free} variables can be built in the following ways:
  \begin{itemize}
    \item[$\vert$] $\bot$ is an atomic preformula with $n$ free variables
          (and missing nothing).
    \item[$\vert$]
          Given terms $t, s$ with $n$ variables,
          $t = s$ is a formula with $n$ free variables (missing nothing).
    \item[$\vert$] Any relation symbol $r \hookr A^{l}$ is a preformula
          with $n$ free variables and missing $l$ inputs.
          \[ r (?, \cdots, ?)\]
    \item[$\vert$] If $\phi$ is a preformula with $n$ free variables that is missing
          $l + 1$ inputs and $t$ is a term with $n$ variables
          then we can \textit{apply} $\phi$ to $t$, obtaining
          a preformula that is missing $l$ inputs.
          \[ \phi(t , ? , \cdots, ? )\]
    \item[$\vert$] If $\phi$ and $\psi$ are preformulas with $n$ free variables
          and \textit{nothing missing} then so is $\phi \implies \psi$.
    \item[$\vert$] If $\phi$ is a preformula with $n + 1$ free variables
          and \textit{nothing missing} then $\forall x_{0}, \phi$ is a preformula
          with $n$ free variables and nothing missing.
  \end{itemize}

  We take formulas to be preformulas with nothing missing.
  Note that we take the de Brujn index convention here.
  If $\phi$ were the formula $x_{0} + x_{1} = x_{2}$ then
  $\forall x_{0}, \phi$ would be the formula $\forall x_{0} : A, x_{0} + x_{1} = x_{2}$,
  which is really $\forall x : A, x + x_{0} = x_{1}$,
  so that all the remaining free variables are shifted down.

  We define \textit{sentences} as preformulas with $0$ variables and nothing missing.
  Sentences are what we usually come up with when we make assertions.
  For example $x = 0$ is not, but $\forall x : A, x = 0$ \textit{is} an assersion about rings.

  \begin{lstlisting}
    inductive bounded_preformula : ℕ → ℕ → Type u
    | bd_falsum {n : ℕ} : bounded_preformula n 0
    | bd_equal {n : ℕ} (t₁ t₂ : bounded_term L n) : bounded_preformula n 0
    | bd_rel {n l : ℕ} (R : L.relations l) : bounded_preformula n l
    | bd_apprel {n l : ℕ} (f : bounded_preformula n (l + 1)) (t : bounded_term L n) : bounded_preformula n l
    | bd_imp {n : ℕ} (f₁ f₂ : bounded_preformula n 0) : bounded_preformula n 0
    | bd_all {n : ℕ} (f : bounded_preformula (n+1) 0) : bounded_preformula n 0

    def bounded_formula (n : ℕ) := bounded_preformula L n 0
    def sentence := bounded_preformula L 0 0\end{lstlisting}

  Since we are working with classical logic we
  make everything else we need by use of the excluded middle\footnote{
    Or rather, we \textit{will} need excluded middle once we start to
    interpret these sentences.
  }:

  \begin{lstlisting}
    /-- ⊥ is for bd_falsum, ≃ for bd_equal, ⟹ for bd_imp, and ∀' for bd_all -/
    /-- we will write ~ for bd_not, ⊓ for bd_and, and infixr ⊔ for bd_or -/
    def bd_not {n} (f : bounded_formula L n) : bounded_formula L n := f ⟹ ⊥
    def bd_and {n} (f₁ f₂ : bounded_formula L n) : bounded_formula L n := ~(f₁ ⟹ ∼f₂)
    def bd_or {n} (f₁ f₂ : bounded_formula L n) : bounded_formula L n := ~f₁ ⟹ f₂
    def bd_ex {n} (f : bounded_formula L (n+1)) : bounded_formula L n := ~ (∀' ~ f))
  \end{lstlisting}
\end{dfn}

\begin{rmk}[Induction principles]
  Each inductive type is automatically given a mapping out property (or induction principle or recursor)
  \footnote{
    Due to \code{lean}'s type theory having a universe of propositions \code{Prop},
    there is a slight difference between recursion and induction,
    the former mapping into \textit{types} and the latter mapping into \textit{propositions}.
    However, for the purposes of understanding they can be identified.
  },
  and this is central for any constructions and proofs involving terms and formulas.
  Preterms and preformulas are quite elaborate inductive types,
  due to \code{bd\_app}, \code{bd\_apprel} and \code{bd\_all},
  and much more complicated than \code{nat} and \code{list T}.\footnote{
    The recursors for \linkto{nat_dfn}{the naturals} and \linkto{list_dfn}{lists}
    are explained in the appendix.
    The difference in wording there should reflect that they are recursors
    rather than induction principles.
  }

  The induction principle for terms states that to prove a statement about
  bounded preterms missing \textit{any number of inputs},
  it suffices to prove the statement for the variables,
  to prove the statement for \code{bd\_func f},
  and to prove the statement for \code{bd\_app t s} given proofs for the statement
  for preterms \code{t} and \code{s} respectively.

  This induction requires working with preterms missing all $l$ inputs at once.
  This is because if \code{t} is a preterm with $n$ variables and $l + 1$ missing inputs,
  then \code{bd\_app t s} is a preterm with $n$ variables and $l$ missing inputs;
  we needed information from \code{bounded\_preterm n (l + 1)} to deduce
  information for something of type \code{bounded\_preterm n l}.

  The induction principle for formulas is similar,
  with \code{bd\_falsum} and \code{bd\_equal} behaving like \code{bd\_var},
  \code{bd\_rel} behaving like \code{bd\_func},
  and \code{bd\_apprel} behaving like \code{bd\_app}.
  For the rest of the induction,
  one must prove the statement about \code{bd\_imp f1 f2} given
  proofs of the statement for preformulas \code{f1} and \code{f2},
  and prove the statement about \code{bd\_all f} given a proof
  of the statement for \code{f}.

  This induction requires not only to work with preformulas missing all $l$ inputs at once,
  but also to work with preformulas in all $n$ variables at once.
  This is because if \code{f} is a preformula with $n + 1$ variables missing $l$ inputs then
  then \code{bd\_all f} is a preformula with $n$ variables missing $l$ inputs;
  we needed information from \code{bounded\_preformula (n + 1) l} to deduce information
  for something of type \code{bounded\_preformula (n + 1) l}.

\end{rmk}

\subsection{Lean symbols for ring symbols}
\link{lean_symbols_for_ring_symbols}

We define \code{bounded\_ring\_term} and \code{bounded\_ring\_formula}
for convenience.

\begin{lstlisting}
def bounded_ring_formula (n : ℕ) := bounded_formula ring_signature n
def bounded_ring_term (n : ℕ) := bounded_term ring_signature n\end{lstlisting}

We supply instances of \code{has\_zero}, \code{has\_one}, \code{has\_neg},
\code{has\_add} and \code{has\_mul} to each type of bounded ring terms
\code{bounded\_ring\_terms n}.
This way we can use the lean symbols when writing terms and formulas.

\begin{lstlisting}
instance bounded_ring_term_has_zero {n} :
  has_zero (bounded_ring_term n) := ⟨ bd_func ring_consts.zero ⟩

instance bounded_ring_term_has_one {n} :
  has_one (bounded_ring_term n) := ⟨ bd_func ring_consts.one ⟩

instance bounded_ring_term_has_neg {n} : has_neg (bounded_ring_term n) :=
⟨ bd_app (bd_func ring_unaries.neg) ⟩

instance bounded_ring_term_has_add {n} : has_add (bounded_ring_term n) :=
⟨ λ x, bd_app (bd_app (bd_func ring_binaries.add) x) ⟩

instance bounded_ring_term_has_mul {n} : has_mul (bounded_ring_term n) :=
⟨ λ x, bd_app (bd_app (bd_func ring_binaries.mul) x) ⟩\end{lstlisting}

Note that the above fixes which order
addition and multiplication work in (symbolically),
but this won't matter once we interpret into commutative rings.

Since we have multiplication, we also can take terms to powers
using \code{npow\_rec}.

\begin{lstlisting}
instance bounded_ring_term_has_pow {n} : has_pow (bounded_ring_term n) ℕ :=
⟨ λ t n, npow_rec n t ⟩
\end{lstlisting}

We can now write down the sentences that describe rings with ease.
\link{sentences_for_ring_theory}

\begin{lstlisting}
  /-- Associativity of addition -/
  def add_assoc : sentence ring_signature :=
  ∀' ∀' ∀' ( (x_ 0 + x_ 1) + x_ 2 ≃ x_ 0 + (x_ 1 + x_ 2) )

  /-- Identity for addition -/
  def add_id : sentence ring_signature := ∀' ( x_ 0 + 0 ≃ x_ 0 )

  /-- Inverse for addition -/
  def add_inv : sentence ring_signature := ∀' ( - x_ 0 + x_ 0 ≃ 0 )

  /-- Commutativity of addition-/
  def add_comm : sentence ring_signature := ∀' ∀' ( x_ 0 + x_ 1 ≃ x_ 1 + x_ 0 )

  /-- Associativity of multiplication -/
  def mul_assoc : sentence ring_signature :=
  ∀' ∀' ∀' ( (x_ 0 * x_ 1) * x_ 2 ≃ x_ 0 * (x_ 1 * x_ 2) )

  /-- Identity of multiplication -/
  def mul_id : sentence ring_signature :=  ∀' ( x_ 0 * 1 ≃ x_ 0 )

  /-- Commutativity of multiplication -/
  def mul_comm : sentence ring_signature := ∀' ∀' ( x_ 0 * x_ 1 ≃ x_ 1 * x_ 0   )

  /-- Distributivity -/
  def add_mul : sentence ring_signature :=
  ∀' ∀' ∀' ( (x_ 0 + x_ 1) * x_ 2 ≃ x_ 0 * x_ 2 + x_ 1 * x_ 2 )\end{lstlisting}

We later collect all of these into one set and call it the
\linkto{ring_theory}{theory of rings}.

\subsection{Interpretation of symbols}

In the above we set up a symbolic treatment of logic.
In this subsection we try to make these symbols into tangible mathematical objects.

We intend to apply the statement
``any degree two polynomial over our ring has a root''
to a real, usable, tangible ring.
We would like the sort symbol $A$ to be interpreted as the underlying type
for the ring and the function symbols to actually become maps from the ring to itself.

\begin{dfn}[Structures]
    Given a language \code{L}, an \code{L}-\textit{structure} \code{M}
    interpreting \code{L} consists of the following
    \begin{itemize}
      \item An underlying type \code{carrier}.
      \item Each function symbol $f : A^{n} \to A$ is interpreted as a
            function that takes an $n$-ary tuple in \code{carrier}
            to something of type \code{carrier}.
            Note that the domain of this function for constant symbols is an empty tuples,
            hence it is a constant map into \code{carrier},
            or simply a constant of type \code{carrier}.
      \item Each relation symbol $r \hookr A^{n}$
            is interpreted as a proposition about $n$-ary tuples in \code{carrier},
            which can also be viewed as the subset of the set of $n$-ary tuples
            satisfying that proposition.
    \end{itemize}

  \begin{lstlisting}
  structure Structure (L : Language) :=
  (carrier : Type u)
  (fun_map : ∀{n}, L.functions n → dvector carrier n → carrier)
  (rel_map : ∀{n}, L.relations n → dvector carrier n → Prop)\end{lstlisting}

  The \code{flypitch} library defines \code{dvector A n} to be
  $n$-ary tuples of terms in \code{A}.
  This is replaced with \code{fin} in the \code{mathlib} setup of model theory.

  Note that \code{Structure L}, like \code{Language},
  is itself a mathematical \code{structure}.
  This is sensible, since \code{Structure} is meant to generalize algebraic
  and relational mathematical structures such as rings, modules, and graphs.
\end{dfn}

The structures in a language will become the models of \linkto{dfn_theory}{theories}.
For example $\Z$ is a structure in the \linkto{dfn_rings}{language of rings},
a model of the \linkto{ring_theory}{theory of rings} but not a model of the theory of fields.
In the language of \linkto{dfn_bin_rel}{binary relations},
$\N$ with the usual ordering $<$ is a structure that models of
the theory of partial orders but not the theory of equivalence relations.

Before continuing to formalize ``any degree two polynomial over our ring has a root'',
we stop to notice that structures in a language form suitable objects for a category.

\begin{dfn}[\code{L}-morphism, \code{L}-embedding]
    \link{category_of_structures}
    The collection of all \code{L}-structures forms a category with objects
    as \code{L}-structures and morphisms as \code{L}-morphisms.

    \begin{lstlisting}
protected structure hom :=
(to_fun : M → N)
(map_fun' : ∀{n} (f : L.functions n) x, to_fun (M.fun_map f x)
  = N.fun_map f (dvector.map to_fun x) . obviously)
(map_rel' : ∀{n} (r : L.relations n) x, M.rel_map r x
  → N.rel_map r (dvector.map to_fun x) . obviously)\end{lstlisting}

    The induced map between the $n$-ary tuples is called \code{dvector.map}.
    The above says a morphism is a mathematical structure
    consisting of three pieces of data.
    The first says that we have a functions between the carrier types,
    the second gives a sensible commutative diagram for functions,
    and the last gives a sensible commutative diagram for relations\footnote{
      The way to view relations on a structure categorically is to view it
      as a subobject of the carrier type.}.

    \begin{cd}
      \code{dvector M.carrier n}
      \ar[r, "\code{M.fun\_map}"] \ar[d, "\code{dvector.map to\_fun}", swap]
      & \code{M.carrier} \ar[d, "\code{to\_fun}"]\\
      \code{dvector N.carrier n}
      \ar[r, "\code{N.fun\_map}"] & \code{N.carrier}\\
        \mmintp{r}
        \ar[hookrightarrow]{r} \ar[d, "\code{dvector.map to\_fun}", swap]
        & \code{dvector M.carrier n}
        \ar[d, "\code{dvector.map to\_fun}"]\\
        \nnintp{r}
        \ar[hookrightarrow]{r} & \code{dvector N.carrier n}
      \end{cd}
\end{dfn}

Returning to our objective,
we realize that we need to interpret our polynomial (a term)
as something in our ring. The term

\[ x_{0} x_{3}^{2} + x_{1} x_{3} + x_{2} \]
Should be a map sending $4$-tuples in the ring to a single value in the ring,
namely, taking $(a, b, c, d)$ to

\[ a c^{2} + b c + d \]

Thus terms should be interpreted as maps $A^{n} \to A$ for an $L$-structure $A$.

\begin{dfn}[Interpretation of terms]
    \link{interpretation_terms}
    Given \code{L}-structure $\code{M}$ and a \code{L}-term $t$ with up to $n$-variables.
    Then we can naturally interpret (or realize) $t$ in the \code{L}-structure $\code{M}$ as a
    map from the $n$-tuples of $\code{M}$ to $\code{M}$ that
    commutes with the interpretation of function symbols.

    \begin{lstlisting}
@[simp] def realize_bounded_term {M : Structure L} {n} (v : dvector M n) :
  ∀{l} (t : bounded_preterm L n l) (xs : dvector M l), M.carrier
| _ (x_ k)         xs := v.nth k.1 k.2
| _ (bd_func f)    xs := M.fun_map f xs
| _ (bd_app t₁ t₂) xs := realize_bounded_term t₁ (realize_bounded_term t₂ ([])::xs) \end{lstlisting}

    This is defined by recursion on (pre)terms.
    When the preterm $t$ is a variable $x_{k}$, we interpret $t$ as a map
    that picks out the $k$-th part of the $n$-tuple \code{xs}.
    This is like projecting to the $k$-th axis when we view $\code{M}^{n}$ as an affine $n$-space.
    When the term is a function symbol, then we automatically get a map from the
    definition of structures.
    In the last case we are applying a preterm $t_{1}$ to a term $t_{2}$,
    and by induction we already have interpretation of these two preterms
    in our structure, so we compose these in the obvious way.
\end{dfn}

This completes our goal of conveying
``any (at most) degree two polynomial in our ring has a root''.
Whilst terms are interpreted as maps, formulas are interpreted as propositions,
asking if the structure \code{M} satisfies the proposition.

\begin{dfn}[Interpretation of formulas]

    Given \code{L}-structure $\code{M}$ and a \code{L}-formula $f$ with up to $n$-variables.
    Then we can interpret (a.k.a realize or satisfy) $f$ in the \code{L}-structure $\code{M}$ as a
    proposition about $n$ terms from the carrier type.

    \begin{lstlisting}
@[simp] def realize_bounded_formula {M : Structure L} :
  ∀{n l} (v : dvector M n) (f : bounded_preformula L n l) (xs : dvector M l), Prop
| _ _ v bd_falsum       xs := false
| _ _ v (t₁ ≃ t₂)       xs := realize_bounded_term v t₁ xs = realize_bounded_term v t₂ xs
| _ _ v (bd_rel R)      xs := M.rel_map R xs
| _ _ v (bd_apprel f t) xs := realize_bounded_formula v f (realize_bounded_term v t ([])::xs)
| _ _ v (f₁ ⟹ f₂)       xs := realize_bounded_formula v f₁ xs → realize_bounded_formula v f₂ xs
| _ _ v (∀' f)          xs := ∀(x : M), realize_bounded_formula (x::v) f xs \end{lstlisting}

  This is defined by induction on (pre)formulas.
  \begin{itemize}
    \item[$\vert$] $\bot$ is interpreted as the type theoretic proposition \code{false}.
    \item[$\vert$] $t = s$ is interpreted as type theoretic equality of the interpreted terms.
    \item[$\vert$] Interpretation of relation symbols is part of the data of an
          \code{L}-structure (\code{rel\_map}).
    \item[$\vert$] If $f$ is a preformula with $n$ free variables that is missing
          $l + 1$ inputs and $t$ is a term with $n$ variables
          then $f$ applied to $t$ can be interpreted using the interpretation of $f$ and
          applied to the interpretation of $t$, both of which are given by induction.
    \item[$\vert$] An implication can be interpreted as a type theoretic implication
          using the inductively given interpretations on each formula.
    \item[$\vert$] $\forall x_{0}, f$ can be interpreted as the type theoretic proposition
          ``for each $x$ in the carrier set $P$'',
          where $P$ is the inductively given interpretation.
  \end{itemize}

  We write $\code{M} \models f(a)$ to mean ``the realization of $f$ holds in $\code{M}$ for the terms $a$''.
  We are particularly interested in the case when the formula is a sentence,
  which we denote as $\code{M} \vDash f$ (since we need no terms).
  \begin{lstlisting}
@[reducible] def realize_sentence (M : Structure L) (f : sentence L) : Prop :=
realize_bounded_formula ([] : dvector M 0) f ([])\end{lstlisting}
\end{dfn}

\subsection{Theories}

A ring should be exactly a structure in the language of rings satisfying the
sentences that axiomatize rings.
In this subsection we organize algebraic definitions such as rings, fields,
and algebraically closed fields in terms of the sentences that axiomatize them.
We call these theories.

Note that this is \textit{the whole point of model theory},
we will be able to work with terms, formulas, structures and theories tangibly
as mathematical objects rather than in a meta-theory.
This allows us to reason about logic itself,
and its interaction with the real world.

\begin{dfn}[Theories and models]
  \link{dfn_theory}
  Given a language \code{L},
  a set of sentences in the language is a theory in that language.
  \begin{lstlisting}
    def Theory := set (sentence L)   \end{lstlisting}

    Given an \code{L}-structure $\code{M}$ and \code{L}-theory $\code{T}$,
    we write $\code{M} \vDash \code{T}$ and say
    \emph{$\code{M}$ is a model of $\code{T}$} when
    for all sentences $f \in \code{T}$ we have $\code{M} \vDash f$.

    \begin{lstlisting}
      def all_realize_sentence (M : Structure L) (T : Theory L) := ∀ f, f ∈ T → M ⊨ f \end{lstlisting}

    We say an \code{L}-theory is consistent if it has a model.
  \end{dfn}

\begin{dfn}[The theories of rings, fields and algebraically closed fields]
  \link{ring_theory}

  The theory of rings is just the set of the
  \linkto{sentences_for_ring_theory}{sentences describing a ring}.
  A model of the \linkto{ring_theory}{theory of rings} should be exactly the data of a ring.
  We will show this in the \linkto{algebraic_objects_iff_models}{next section}.

\begin{lstlisting}
  def ring_theory : Theory ring_signature :=
  {add_assoc, add_id, add_inv, add_comm, mul_assoc, mul_id, mul_comm, add_mul}\end{lstlisting}

  To make the theory of fields we can add two sentences saying that
  the ring is non-trivial and has multiplicative inverses:
  \begin{lstlisting}
    def mul_inv : sentence ring_signature :=
    ∀' (x_ 1 ≃ 0) ⊔ (∃' x_ 1 * x_ 0 ≃ 1)

    def non_triv : sentence ring_signature := ~ (0 ≃ 1)

    def field_theory : Theory ring_signature := ring_theory ∪ {mul_inv , non_triv} \end{lstlisting}

  To make the theory of algebraically closed fields we need to express
  ``every non-constant polynomial has a root''.
  We replace this with the equivalent statement ``every monic polynomial has a root''.
  We do this by first making ``generic polynomials''
  in the form of $a_{n+1}x^{n} + \cdots + a_{2}x + a_{1}$,
  then adding $x^{n+1}$ to it, making it a ``generic monic polynomial''.
  The (polynomial) variable $x$ will be represented by the variable \code{x\_ 0},
  and the coefficient $a_{k}$ for each $0 < k$ will be represented by the variable
  \code{x\_ k}.

  We define generic polynomials of degree (at most) $n$ as bounded ring signature terms
  in $n + 2$ variables by induction on $n$:
  when the degree is $0$, we just take the constant polynomial $x_{1}$
  and supply a proof that $1 < 0 + 2$ (we omit these below using underscores).
  When the degree is $n + 1$, we can take the previous generic polynomial,
  lift it up from a term in $n + 2$ variables to $n + 3$ variables
  (this is \code{lift\_succ}),
  then add $x_{n + 2} x_{0}^{n+1}$ at the front.

  \begin{lstlisting}
    def gen_poly : Π (n : ℕ), bounded_ring_term (n + 2)
    | 0       := x_ ⟨ 1 , _ ⟩
    | (n + 1) := (x_ ⟨ n + 2 , _ ⟩) * (npow_rec (n + 1) (x_ ⟨ 0 , _ ⟩))
                   + bounded_preterm.lift_succ (gen_poly n)\end{lstlisting}

  Since the type of terms in the language of rings has notions of
  addition and multiplication (using the function symbols),
  we automatically have a way of taking (natural number) powers.
  This is \code{npow\_rec}.

  We proceed to making generic monic polynomials by adding
  $x_{0}^{n+2}$ at the front of the generic polynomial.

  \begin{lstlisting}
  def gen_monic_poly (n : ℕ) : bounded_term ring_signature (n + 2) :=
  npow_rec (n + 1) (x_ 0) + gen_poly n

  /-- ∀ a₁ ⋯ ∀ aₙ, ∃ x₀, (aₙ x₀ⁿ⁻¹ + ⋯ + a₂ x₀+ a₁ = 0) -/
  def all_gen_monic_poly_has_root (n : ℕ) : sentence ring_signature :=
  fol.bd_alls (n + 1) (∃' gen_monic_poly n ≃ 0) \end{lstlisting}

  We can then easily state ``all generic monic polynomials have a root''.
  The order of the variables is important here:
  the $\exists$ removes the first variable $x_{0}$ in the $n+2$ variable formula
  $\code{gen\_monic\_poly n} \simeq 0$, and moves the index of all the
  variables down by $1$, making the remaining expression
  \[\exists \, \code{gen\_monic\_poly n} \simeq 0\] a formula in $n+1$ variables.
  The function \code{fol.bd\_alls n} then adds $n+1$ many ``foralls''
  in front, leaving us a formula with no free variables, i.e. sentence.

  \begin{lstlisting}
  /-- The theory of algebraically closed fields -/
  def ACF : Theory ring_signature :=
  field_theory ∪ (set.range all_gen_monic_poly_has_root)\end{lstlisting}

  Since \code{all\_gen\_monic\_poly\_has\_root} is a function from the naturals,
  we can take its set theoretic image (called \code{set.range}),
  i.e. a sentence for each degree $n$ saying
  ``any monic polynomial of degree $n$ has a root''.

  Lastly, we express the characteristic of fields,
  which is important since $\ACF$ is not a \linkto{dfn_complete_theory}{complete theory},
  but adding in sentences specifying characteristic makes it complete for any $p$ ($0$ or prime).
  Characteristic considerations also allow us to reduce to proving the theorem for
  \linkto{locally_finite}{locally finite fields}.

  Suppose $p : \N$ is a prime.
  If we view $p$ as a term in the language of rings\footnote{
    \code{lean} figures this out automatically using \code{nat.cast},
    which found our instances of \code{has\_zero}, \code{has\_one}
    and \code{has\_add}.
  },
  then we can define the theory of algebraically closed fields of characteristic $p$
  as \code{ACF} with the additional sentence $p = 0$.

  \begin{lstlisting}
  def ACFₚ {p : ℕ} (h : nat.prime p) : Theory ring_signature :=
  set.insert (p ≃ 0) ACF\end{lstlisting}

  To define the theory of algebraically closed fields of characteristic $0$,
  we add a sentence $n + 1 \ne 0$ for each natural $n$.

  \begin{lstlisting}
  def plus_one_ne_zero (n : ℕ) : sentence ring_signature :=
  ¬ (n + 1 ≃ 0)

  def ACF₀ : Theory ring_signature := ACF ∪ (set.range plus_one_ne_zero)  \end{lstlisting}

\end{dfn}

  Whilst completeness and soundness for first order logic
  is about converting between symbolic and semantic deduction,
  there is another layer of conversion that is often swept under the rug,
  between the internal semantics and the ambient mathematics,
  which we will informally call ``internal completeness and soundness''.
  For example, we need to show that a model of a ring is
  actually a ring, according to the \code{mathlib} definition of a ring.
  Showing that models of structures are equivalent to their ``actual'' mathematical counterparts
  and that model-theoretically stated theorems are equivalent to
  their counterparts is arguably the most important part of the project:
  it shows that model theory actually produces results that are usable
  in other areas of maths.

% \begin{proof}
%   The proof of this formed a significant part of this project.
%   We leave this to the next two sections.
% \end{proof}

% \begin{dfn}[Consequence]
%     Given a \code{L}-theory $T$
%     and a \code{L}-sentence $\phi$,
%     we say $\phi$ is a consequence of $T$
%     and say $T \model{\code{L}} \phi$
%     when for all \code{L}-models $\code{M}$ of $T$,
%     we have $\code{M} \model{\code{L}} \phi$.
%     We also write $T \model{\code{L}} \De$
%     for \code{L}-theories $T$ and $\De$
%     when for every $\phi \in \De$ we have $T \model{\code{L}} \phi$.
% \end{dfn}

% \begin{ex}[Logical consequence]
%     Let $T$ be a \code{L}-theory and $\phi$ and $\psi$ be \code{L}-sentences.
%     Show that the following are equivalent:
%     \begin{itemize}
%         \item $T \model{\code{L}} \phi \to \psi$
%         \item $T \model{\code{L}} \phi$ implies $T \model{\code{L}} \psi$.
%     \end{itemize}
% \end{ex}

% \begin{dfn}[Consistent theory]
%     \link{consistent}
%     A \code{L}-theory $T$ is consistent if either of the following equivalent
%     definitions hold:
%     \begin{itemize}
%         \item
%             There does not exists a
%             \code{L}-sentence $\phi$ such that
%             $T \model{\code{L}} \phi$ and $T \model{\code{L}} \NOT \phi$.
%         \item There exists
%             a \code{L}-model of $T$.
%     \end{itemize}
%     Thus the definition of consistent is intuitively
%     `$T$ does not lead to a contradiction'.
%     A theory $T$ is finitely consistent if all
%     finite subsets of $T$ are consistent.
%     This will turn out to be another equivalent definition,
%     given by the \linkto{compactness}{compactness theorem}.
% \end{dfn}
% \begin{proof}
%     We show that the two definitions are equivalent.
%     \begin{forward}
%         Suppose no model exists.
%         Take $\phi$ to be the \code{L}-sentence $\top$.
%         Hence all \code{L}-models of $T$ satisfy $\top$ and $\bot$
%         (there are none) so
%         $T \model{\code{L}} \top$ and $T \model{\code{L}} \bot$.
%     \end{forward}
%     \begin{backward}
%         Suppose $T$ has a \code{L}-model $\code{M}$
%         and $T \model{\code{L}} \phi$ and $T \model{\code{L}} \NOT \phi$.
%         This implies $\code{M} \model{\code{L}} \phi$ and $\code{M} \nodel{\code{L}} \phi$,
%         a contradiction.
%     \end{backward}
% \end{proof}

% \begin{dfn}[Elementary equivalence]
%     Let $\code{M}$, $\NN$ be \code{L}-structures.
%     They are elementarily equivalent if for any \code{L}-sentence $\phi$,
%     $\code{M} \model{\code{L}} \phi$ if and only if $\NN \model{\code{L}} \phi$.
%     We write $\code{M} \equiv_\code{L} \NN$.
% \end{dfn}

% \begin{dfn}[Maximal and complete theories]
%     \link{equiv_def_completeness_0}
%     A \code{L}-theory $T$ is \textit{maximal} if
%     for any \code{L}-sentence $\phi$,
%     $\phi \in T$ or $\NOT \phi \in T$.

%     $T$ is \textit{complete}
%     when either of the following equivalent
%     definitions hold:
%     \begin{itemize}
%         \item For any \code{L}-sentence $\phi$,
%             $T \model{\code{L}} \phi$ or
%             $T \model{\code{L}} \NOT \phi$.
%         \item All models of $T$ are elementarily equivalent.
%     \end{itemize}
%     Note that maximal theories are complete.
% \end{dfn}
% \begin{proof}
%     \begin{forward}
%         Let $\code{M}$ and $\NN$ be models of $T$
%         and $\phi$ be a \code{L}-sentence.
%         If $T \model{\code{L}} \phi$ then both satisfy $\phi$.
%         Otherwise $\NOT \phi \in T$ and neither satisfy $\phi$.
%     \end{forward}

%     \begin{backward}
%         If $\phi$ is a \code{L}-sentence then suppose for a contradiction
%         \[T \nodel{\code{L}} \phi \text{ and } T \nodel{\code{L}} \NOT \phi\]
%         Then there exist models of $T$
%         such that $\code{M} \nodel{\code{L}} \phi$ and $\NN \nodel{\code{L}} \NOT \phi$.
%         By assumption they are elementarily equivalent and so
%         $\code{M} \model{\code{L}} \NOT \phi$ implies $\NN \model{\code{L}} \NOT \phi$,
%         a contradiction.
%     \end{backward}
% \end{proof}

% \begin{ex}[Not consistent, not complete]
%     \link{not_consequence}
%     Let $T$ be a \code{L}-theory
%     and $\phi$ is a \code{L}-sentence.
%     Show that $T \nodel{\code{L}} \phi$
%     if and only if $T \cup \set{ \NOT \phi}$ is consistent.
%     Furthermore, $T \nodel{\code{L}} \NOT \phi$
%     if and only if $T \cup \set{\phi}$ is consistent.

%     Note that by definition for \code{L}-structures and
%     \code{L}-formulas we (classically) have that
%     \[
%         \code{M} \modelsi \NOT \phi(a) \iff \code{M} \nodelsi \phi(a)
%     \]
%     Find examples of theories that do not satisfy
%     \[
%         T \modelsi \NOT \phi \iff T \nodelsi \phi
%     \]
% \end{ex}
